<div id="scrollable_div" style="display: none;" onmouseleave="mf.close_scrollable_div();">
	<div style="float: right">
		<div style=" font-size: 200%; &#10;&#9;&#9;&#9;font-weight: 300%; &#10;&#9;&#9;&#9;float: right; &#10;&#9;&#9;&#9;border-style: solid;&#10;&#9;&#9;&#9;position: fixed;&#10;&#9;&#9;&#9;cursor: pointer; &#10;&#9;&#9;&#9;" onclick="mf.close_scrollable_div();">
			X
		</div>
		     
	</div>
	<div id="scrollable_data"></div>
</div>

<div id="page_container">

	<link id="link_css" rel="stylesheet" type="text/css" href="/mainframe_versus_other_computers/mainframe_versus_linux.css" />

	<script id="execute_script" type="text/javascript">
		mf = {

			open_scrollable_div: function(template_name) {
				scrollable_div.style.display = "inline";
				page_container.style.display = "none";
				// document.body.style.overflowY = "hidden";
				scrollable_data.innerHTML = template_name.innerHTML;
			},

			close_scrollable_div: function () {
				scrollable_div.style.display = "none";
				page_container.style.display = "inline";
				//document.body.style.overflowY = "auto";
			},
			
			display_toc: function () {
				toc_content.style.display = "block";
			},
			
			hide_toc: function () {
				toc_content.style.display = "none";
			}
		}
	</script>

	<div id="toc" onmouseenter="mf.display_toc();" onmouseleave="mf.hide_toc();">
		<div id="toc_title">Table of Contents</div>
		<ul id="toc_content" style="display: block;"><li><a href="#toc_0">It's time to be brutally honest with yourself
Answering why the mainframe still exist</a></li>
<li><a href="#toc_1">Lie #1: Google understands "Distributed computing"</a></li>
<ul>
<li><a href="#toc_2">IRS mainframe modernization
implementing Linux "Distributed computing" concepts</a></li>
<li><a href="#toc_3">Understanding why the IRS will fail
because of non-mainframe "distributed computing" databases</a></li>
</ul><li><a href="#toc_4">Lie #2: Developers on Linux can easily use "distributed computing"</a></li>
<ul>
</ul><li><a href="#toc_5">List of future additions to this document</a></li>
<ul>
</ul></ul>

	
	</div>

<br />

<h1>Under construction and incomplete</h1>

<p>New lies to be added here after they are discussed on IBM-MAIN.</p>

<h1>Linux software developers still believe all the lies
	<br />Understanding Google's lack of basic computer skills</h1>

	<p>Like Google, you still believe decades of lies about the mainframe. In particular, you incorrectly believe it's an obsolete dinosaur. You incorrectly believe Google is a successful computer company because of their computer expertise but sadly you ignore that they don't understand most simple basic computer concepts. Not 1 IBM mainframe at Google despite 5,000,000 servers, 182,000 employees and tons of software they designed and wrote (e.g. Android, ChromeOS, Go, Chrome browser, Waymo and much more).</p>

	<p><a href="https://www.investing.com/news/analyst-ratings/bernstein-survey-shows-cios-focused-on-software-and-cloud-with-weak-sentiment-on-mainframes-93CH-3761586" class="center-container" target="mainframe_topic"><img height="300" src="/mainframe_versus_other_computers/mainframe_decline.png" /></a></p>
	<hr style="display: none;" />
	
	<h2 id="toc_0">It's time to be brutally honest with yourself<br />Answering why the mainframe still exist</h2>

	<p>My focus here are the <b>facts</b> about Linux features that Linux software developers incorrectly believe, makes Linux an outstanding success. Since you'll never understand the mainframe features, let's forget about discussing mainframe features and instead, focus on the design failures of C and Linux.</p>

	<p>Linux has been available on the mainframe for the last 25 years, so <span class="fake_anchor" onclick="mf.open_scrollable_div(telum_grok);">I asked the GROK AI, "Is the IBM Telum the fastest CPU?".</span> The Answer was horrifying. A quarter century of Linux on the mainframe and computer professionals still believe the same lies they used to justify their expertise in the past.</p>

	<div class="center-container"><img src="/mainframe_versus_other_computers/lemmings.gif" /></div>

	<p><span class="fake_anchor" onclick="mf.open_scrollable_div(telum_grok);">GROK boasted about wonderful mainframe features (E.g. 100,000 transactions per second)</span> but failed to mention that Linux on the mainframe doesn't provide those same features despite being on the same hardware.</p>
	
	<p>On the mainframe, Linux and C are Broken As Designed (BAD). IBM RedHat Linux is now closed source which begs the question, will they fix Linux for the mainframe. It's time to stop putting lipstick on this pig and see the blemishes. It's time for computer professors to teach real computer fundamentals.</p>

	<div class="center-container"><img src="/mainframe_versus_other_computers/lipstick_on_a_pig.jpg" /></div>

	<br /><hr /><h2 id="toc_1">Lie #1: Google understands "Distributed computing"</h2>

	<p><b>ROTFLOL</b> (Rolling On The Floor, Laughing Out Loud)! When you can move a database to an NAS (network disk drive) and have 2 or more database servers updating that database disk, you will finally understand "distributed computing". Google with it's 5,000,000 servers is still limited to 1 database server using 1 database disk drive.</p>
	
	<div class="center-container"><img src="/mainframe_versus_other_computers/bless_your_heart.jpg" width="400px" /></div>
	
	<p>Since 1964 (60 years), IBM DB2 has supported multiple database servers using a shared database disk. When another database server is needed, then you simply add another mainframe computer connected to the same database disks. With Google's extensive knowledge of Linux, why are they still in the dark ages of computing with their 5,000,000 servers?</p>

	<p>Linux mentality has created an endless list of crappy "distributed computing" solutions that fail to deliver on their promises (e.g. kubernetes, "The Cloud", microservices, pub/sub, grid, NFS, consumer &amp; suppliers and too many more to list here). Surely another will be coming next week. How many more attempts will it take for Linux to solve the "Distributed computing" problem?</p>

	<p>35 years ago, the IBM mainframe created "Sysplex" to solve the "distributed computing" problem. A single well designed solution which Linux could have implemented but instead, chooses crappy solutions that never fully solve the problem.</p>

	<p>While you say Linux solutions exist on the mainframe, remember they exist solely for Linux "distributed computing" which lack Sysplex features.</p>

	<h2 id="toc_2">IRS mainframe modernization 
		<br />implementing Linux "Distributed computing" concepts</h2>

	<p><b>ROTFLOL!</b>The IRS is currently "modernizing" their mainframe using JAVA and the Linux "distributed computing" database solution (JDBC) instead of Sysplex. JAVA was designed using Linux / Unix concepts which is why IBM discounts computers for customers using JAVA. They expect to sell more mainframes to those customers to offset the discount. They also realize these applications will remain on the mainframe (or mainframe for Linux) because moving to x86 will most likelyy require a re-write.</p>

	<h2 id="toc_3">Understanding why the IRS will fail
		<br />because of non-mainframe "distributed computing" databases</h2>

	<p>In Linux, database disks cannot be used by 2 different database servers at the same time otherwise they will be destroyed. Once the database server becomes too busy, you are forced into creative solutions.</p>

	<p>On the other hand because of Sysplex, mainframe database servers can run several database servers using the same database disks. If you ran 10 mainframe's serving the same database and assuming IBM is correct that each is the equivalent of 2,000 x86 cpu cores, then that would be 20,000 x86 cores or 1,000 x86 servers.</p>
	
	<div class="center-container"><img src="/mainframe_versus_other_computers/lipstick_on_a_pig.jpg" /></div>

	<hr /><h2 id="toc_4">Lie #2: Developers on Linux can easily use "distributed computing"</h2>
	
	<p>ROTFLOL! On the mainframe, there is no training (or discussion) for "distristributed computing". It's like breathing because because every program you write is 'distibuted computing'. It's only when Linux is involved that you must be taught that skill.</p>

	<hr /><h2 id="toc_5">List of future additions to this document</h2>
	
	<p>C language portability
		<br />C language efficient
		<br />application programmers versus systems programmers
		<br />CPU's are the same
		<br />SMP/e instead of apt
		<br />Why oorexx (object REXX) is not an option
		<br />instruction languages riscv, x86, IBM Z
		<br />wwhy IBM says 256 equals 2,000 x86 cores
		<br />HPE El Capitan supercomputer is same as Googles server farm
		<br />El Capitan 2 computers for each graphics card - they don't understand distributed computing
		<br />IBM &amp; Nvidia partnering  
		<br />How many computers needed for 1 quantum computer 
		<br /> blades
	</p>


<template id="telum_grok">
	<h1>I asked X Grok (AI) "Is IBM Telum the fastest CPU?"<br />
		I was horrified by the lies people still believe</h1>
	<hr />
	<h2>Determining whether the IBM Telum CPU is "faster than all others" depends on the context—specifically, what "faster" means (e.g., clock speed, throughput, latency, or specialized workloads) and which CPUs it’s being compared to. The Telum, introduced in 2021 for IBM’s z16 mainframe systems, is a highly specialized processor designed for enterprise workloads like financial transactions, real-time AI inference (e.g., fraud detection), and extreme reliability. It’s not a general-purpose CPU like those in consumer PCs or even most data center servers, so a direct comparison to "all others" requires nuance.</h2>
	<p>The original Telum operates at a base clock speed exceeding 5 GHz (often cited as 5.2 GHz), with eight cores per chip, and scales up to 256 cores across 32 chips in a full mainframe system. Each core has a massive 32 MB private L2 cache, which can act as a virtual L3 (256 MB) or L4 (2 GB) cache across the system, optimizing low-latency access for high-volume transactional workloads. It also includes a 6 TFLOPS on-chip AI accelerator, boosting performance for specific AI tasks. The newer Telum II, announced in 2024, pushes this further with a 5.5 GHz clock, 36 MB L2 cache per core (360 MB virtual L3, 2.88 GB virtual L4), and a 24 TOPS AI accelerator, claiming up to 70% performance improvement over its predecessor in key system components.</p>
	<h2>Clock Speed Comparison</h2>
	<p>Telum (5.2 GHz) and Telum II (5.5 GHz) have among the highest base clock speeds for server-grade CPUs. Most modern server CPUs, like AMD EPYC (e.g., Zen 4, base ~3.0–4.0 GHz, boost up to 5.7 GHz) or Intel Xeon (e.g., Sapphire Rapids, base ~2.0–3.7 GHz, boost up to 4.1 GHz), prioritize core count over clock speed and rely on turbo boosts that don’t sustain as consistently as Telum’s fixed frequency.</p>
	
	<p>Consumer CPUs, like AMD Ryzen 9 7950X (up to 5.7 GHz boost) or Intel Core i9-14900K (up to 6.0 GHz boost), can exceed Telum’s clock speed but are designed for different workloads (e.g., gaming, single-threaded tasks) and lack the enterprise-grade reliability and scalability of mainframes.</p>
	
	<h2>Throughput and Workload-Specific Performance</h2>
	<p>Telum shines in throughput for transactional workloads. A fully loaded z16 with 256 Telum cores can process tens of thousands of transactions per second with sub-millisecond latency—critical for banking or insurance systems. This is less about raw compute (e.g., FLOPS) and more about consistent, predictable performance. For contrast:</p>
	<p>AMD EPYC 9754 (128 cores, 256 threads, ~3.0 GHz) or Intel Xeon Platinum 8592+ (64 cores, 128 threads, ~3.9 GHz boost) offer higher core counts and raw multi-threaded compute, excelling in HPC or cloud workloads but not necessarily in low-latency, high-reliability transaction processing.</p>
	
	<p>NVIDIA GPUs (e.g., H100, 3000+ INT8 TOPS) or dedicated AI accelerators dwarf Telum’s 24 TOPS for AI training or inference, but they’re not CPUs and don’t handle general-purpose tasks.</p>
	
	<h2>Specialized Features</h2>
	<p>Telum’s on-chip AI accelerator and cache design (virtual L3/L4) give it an edge in real-time AI inference embedded in transactions—something neither x86 CPUs nor consumer chips replicate at the same scale or latency. Its reliability (seven nines uptime, or milliseconds of downtime per year) also outclasses typical server CPUs, which prioritize flexibility over such extreme consistency.</p>
	<h2>Limitations</h2>
	<p>Telum isn’t "faster" in every metric:</p>
	<p>Core Count: With only 8 cores per chip (256 max in a system), it’s dwarfed by EPYC’s 128 or Xeon’s 64+ cores per socket for parallel workloads.</p>
	<p>General Compute: It’s not built for high-performance computing (e.g., scientific simulations) or consumer tasks (e.g., gaming), where x86 or ARM CPUs dominate.</p>
	
	<p>AI Raw Power: Dedicated accelerators like NVIDIA’s H100 or Google’s TPUs far exceed Telum’s AI performance for training or large-scale inference.</p>
	
	<h2>Conclusion</h2>
	<p></p>The IBM Telum (and Telum II) isn’t universally "faster than all others." It’s exceptionally fast—perhaps unmatched—in its niche: high-volume, low-latency, AI-infused transactional workloads with extreme reliability. For general-purpose computing, raw multi-threaded performance, or specialized AI tasks, other CPUs or accelerators (e.g., EPYC, Xeon, GPUs) outperform it. Its speed is tailored to mainframe demands, not a one-size-fits-all metric. If you’re asking about a specific use case or comparison (e.g., Telum vs. EPYC for a workload), let me know, and I can dig deeper!<p></p>
</template>

</div>


